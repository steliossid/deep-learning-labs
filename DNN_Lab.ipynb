{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Laboration\n",
    "\n",
    "Data used in this laboration are from the Kitsune Network Attack Dataset, https://archive.ics.uci.edu/ml/datasets/Kitsune+Network+Attack+Dataset . We will focus on the 'Mirai' part of the dataset. Your task is to make a DNN that can classify if each attack is benign or malicious. The dataset has 116 covariates, but to make it a bit more difficult we will remove the first 24 covariates.\n",
    "\n",
    "You need to answer all questions in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Get the data\n",
    "\n",
    "Use `wget` in the terminal of your cloud machine (in the same directory as where you have saved this notebook) to download the data, i.e.\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_dataset.csv.gz\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_labels.csv.gz\n",
    "\n",
    "Then unpack the files using `gunzip` in the terminal, i.e.\n",
    "\n",
    "gunzip Mirai_dataset.csv.gz\n",
    "\n",
    "gunzip Mirai_labels.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Get a graphics card\n",
    "\n",
    "Lets make sure that our script can see the graphics card that will be used. The graphics cards will perform all the time consuming calculations in every training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning from numpy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "\n",
    "# Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Hardware\n",
    "\n",
    "In deep learning, the computer hardware is very important. You should always know what kind of hardware you are working on.\n",
    "\n",
    "Question 1: What graphics card is available in the cloud machine? Run 'nvidia-smi' in the terminal.\n",
    "\n",
    "Answer 1: According to the command output, the available graphics card is a Tesla K80.\n",
    "\n",
    "Question 2: Google the name of the graphics card, how many CUDA cores does it have?\n",
    "\n",
    "Answer 2: According to the official NVIDIA (https://www.nvidia.com/en-gb/data-center/tesla-k80/) website it has 4992 CUDA cores.\n",
    "\n",
    "Question 3: How much memory does the graphics card have?\n",
    "\n",
    "Answer 3: According to the command output, Tesla K80 has 11441MB of memory.\n",
    "\n",
    "Question 4: What is stored in the GPU memory while training a DNN ?\n",
    "\n",
    "Answer 4:\n",
    "\n",
    "Question 5: What CPU is available in the cloud machine? How many cores does it have? Run 'lscpu' in the terminal.\n",
    "\n",
    "Answer 5: According to the command output, the cloud machine has an Intel Xeon CPU E5-2690 v3 @ 2.60GHz and it has 6 cores.\n",
    "\n",
    "Question 6: How much CPU memory (RAM) is available in the cloud machine? Run 'free -g' in the terminal.\n",
    "\n",
    "Answer 6: According to the command output, the available memory is 53 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Load the data\n",
    "\n",
    "Load the dataset from the csv files, it will take some time since it is almost 1.4 GB. \n",
    "\n",
    "We will use the function `genfromtxt` to load the data.\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\n",
    "\n",
    "Load the data from csv files the first time, then save the data as numpy files for faster loading the next time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "\n",
    "# Load data from file\n",
    "# X = covariates, Y = labels\n",
    "X = genfromtxt(\"Mirai_dataset.csv\", delimiter=\",\")\n",
    "Y = genfromtxt(\"Mirai_labels.csv\", delimiter=\",\")\n",
    "\n",
    "# Save data as numpy arrays, for faster loading in future calls to this cell\n",
    "np.save('Mirai_data.npy', X)\n",
    "np.save('Mirai_labels.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The covariates have size (764137, 93).\n",
      "The labels have size (764137,).\n"
     ]
    }
   ],
   "source": [
    "# Load data from numpy arrays, for faster loading\n",
    "X = np.load('Mirai_data.npy')\n",
    "Y = np.load('Mirai_labels.npy')\n",
    "\n",
    "# Remove the first 24 covariates\n",
    "X = np.delete(X, obj=np.s_[:23], axis=1)\n",
    "# X= X[:,24:116]\n",
    "\n",
    "print('The covariates have size {}.'.format(X.shape))\n",
    "print('The labels have size {}.'.format(Y.shape))\n",
    "\n",
    "# Print the number of examples of each class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: How good is a naive classifier?\n",
    "\n",
    "Question 7: Given the distribution of examples, how high classification performance can a naive classifier obtain? The naive classifier will assume that all examples belong to one class. Note: you do not need to make a naive classifier, this is a theoretical question, just to understand how good performance we can obtain by random chance.\n",
    "\n",
    "Answer 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "[False False False ... False False False]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# It is common to have NaNs in the data, lets check for it. Hint: np.isnan()\n",
    "print(np.isnan(X))\n",
    "print(np.isnan(Y))\n",
    "\n",
    "# Print the number of NaNs (not a number) in the labels\n",
    "np.sum(np.isnan(X))\n",
    "\n",
    "# Print the number of NaNs in the covariates\n",
    "print(np.sum(np.isnan(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Preprocessing\n",
    "\n",
    "Lets do some simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.21702133e-14 -3.19451533e-18 -6.32970181e-14  1.19926356e-13\n",
      "  4.56743018e-15  4.10210037e-14  1.46130975e-13  5.85246484e-16\n",
      " -1.69734859e-14 -3.36915700e-13  1.28688437e-12 -2.69360995e-12\n",
      " -1.10733213e-13 -1.22392702e-13 -1.70649630e-13 -1.02461166e-14\n",
      "  2.50701280e-12  1.47553162e-12  1.08446837e-12 -1.04981959e-13\n",
      "  6.83458762e-14 -1.03373555e-13  5.98825773e-14 -1.02025960e-12\n",
      " -1.68983055e-12 -1.79101143e-12 -1.31828514e-13  4.42580403e-13\n",
      "  6.14635580e-13  5.78048199e-14 -4.92623328e-13 -2.54513072e-12\n",
      "  1.86544900e-13 -1.53444593e-13  1.68079591e-12  9.30041709e-13\n",
      "  1.50738177e-13 -1.15688852e-12 -3.62610361e-13 -1.71390937e-12\n",
      " -2.09264067e-13  1.07161976e-12 -1.45236885e-12 -1.69724579e-14\n",
      " -1.64918984e-16 -5.13444996e-14 -1.02171349e-14 -1.74685907e-15\n",
      "  1.34264921e-13  5.98801969e-14  1.48745574e-17 -4.25442340e-13\n",
      "  5.78079594e-14  1.25638129e-15  1.69449684e-13  1.50725881e-13\n",
      "  2.14439542e-14  3.65457183e-14  1.17260451e-13 -8.82752870e-13\n",
      " -6.34816648e-13 -1.62109649e-12  2.63270303e-13 -7.57215123e-15\n",
      " -2.89395002e-14 -3.90180996e-13 -1.53167085e-12 -9.57913621e-13\n",
      "  2.47411065e-13  2.44200541e-13 -6.73050928e-15  1.07502596e-13\n",
      "  2.58222203e-13 -1.87714601e-13 -1.19882476e-12 -2.17154862e-12\n",
      "  5.48444735e-14  5.46183481e-15  3.71315442e-14  1.47576646e-13\n",
      " -1.62639245e-12 -1.23986972e-13 -1.71744315e-12  5.29956657e-13\n",
      " -3.21442452e-14 -4.59767392e-14  3.56347870e-13 -1.48544246e-12\n",
      " -1.26642728e-13  1.52633871e-13  9.58048710e-14  4.34603426e-14\n",
      " -4.07615740e-14]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Convert covariates to floats\n",
    "X = X.astype(float)\n",
    "\n",
    "# Convert labels to ints\n",
    "Y = Y.astype(int)\n",
    "\n",
    "# Remove mean of each covariate (column)\n",
    "X = X - np.mean(X, axis=0)\n",
    "\n",
    "# Divide each covariate (column) by its standard deviation\n",
    "X = X/np.std(X, axis=0)\n",
    "\n",
    "# Check that mean is 0 and standard deviation is 1 for all covariates, by printing mean and std\n",
    "print(np.mean(X, axis=0))\n",
    "print(np.std(X, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Split the dataset\n",
    "\n",
    "Use the first 70% of the dataset for training, leave the other 30% for validation and test, call the variables\n",
    "\n",
    "Xtrain (70%)\n",
    "\n",
    "Xtemp  (30%)\n",
    "\n",
    "Ytrain (70%)\n",
    "\n",
    "Ytemp  (30%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain has size (534895, 93).\n",
      "Ytrain has size (534895,).\n",
      "Xtemp has size (229242, 93).\n",
      "Ytemp has size (229242,).\n"
     ]
    }
   ],
   "source": [
    "split = int(0.7*X.shape[0])\n",
    "Xtrain = X[:split,:]\n",
    "Xtemp = X[split:,:]\n",
    "Ytrain = Y[:split,]\n",
    "Ytemp = Y[split:,]\n",
    "\n",
    "print('Xtrain has size {}.'.format(Xtrain.shape))\n",
    "print('Ytrain has size {}.'.format(Ytrain.shape))\n",
    "\n",
    "print('Xtemp has size {}.'.format(Xtemp.shape))\n",
    "print('Ytemp has size {}.'.format(Ytemp.shape))\n",
    "\n",
    "# Print the number of examples of each class, for the training data and the remaining 30%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Number of examples per class\n",
    "\n",
    "Question 8: Can we use the dataset as it is? Why not?\n",
    "\n",
    "Answer 8: No, we can't use it as it is, because the rows are not shuffled. The rows are sorted based on the class (first rows with class 0 and then rows with class 1).\n",
    "\n",
    "Lets randomly shuffle the data, to get some examples of each class in training data and in the remaining 30%. Use the function `shuffle` in scikit learn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Randomly shuffle data, to get both classes in training and testing\n",
    "X, Y = shuffle(X, Y)\n",
    "\n",
    "# Divide the data into training and validation/test again\n",
    "split = int(0.7*X.shape[0])\n",
    "Xtrain = X[:split,:]\n",
    "Xtemp = X[split:,:]\n",
    "Ytrain = Y[:split,]\n",
    "Ytemp = Y[split:,]\n",
    "\n",
    "split2 =  int(0.5*Xtemp.shape[0])\n",
    "Xtest = Xtemp[:split2,:]\n",
    "Xvalid = Xtemp[split2:,:]\n",
    "Ytest = Ytemp[:split2,]\n",
    "Yvalid = Ytemp[split2:,]\n",
    "\n",
    "# Print the number of examples of each class, for the training data and the remaining 30%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part 9: Split non-training data data into validation and test\n",
    "Split your non-training data (Xtemp, Ytemp) into 50% validation (Xval, Yval) and 50% testing (Xtest, Ytest), we use a function from scikit learn. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation and test data have size (114621, 93), (114621, 93), (114621,) and (114621,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtest, Xval, Ytest, Yval = train_test_split(Xtemp, Ytemp, test_size=0.5)\n",
    "\n",
    "print('The validation and test data have size {}, {}, {} and {}'.format(Xval.shape, Xtest.shape, Yval.shape, Ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: DNN classification\n",
    "\n",
    "Finish this code to create a first version of the classifier using a DNN. Start with a simple network with 2 dense layers (with 20 nodes each), using sigmoid activation functions. The final dense layer should have a single node and a sigmoid activation function. We start with the SGD optimizer.\n",
    "\n",
    "Relevant functions are\n",
    "\n",
    "`model.add()`, adds a layer to the network\n",
    "\n",
    "`Dense()`, a dense network layer\n",
    "\n",
    "`model.compile()`, compile the model, add \" metrics=['accuracy'] \" to print the classification accuracy during the training\n",
    "\n",
    "`model.fit()`, train the model with some training data\n",
    "\n",
    "`model.evaluate()`, apply the trained model to some test data\n",
    "\n",
    "See https://keras.io/layers/core/ for information on how the `Dense()` function works\n",
    "\n",
    "Import a relevant cost / loss function for binary classification from keras.losses (https://keras.io/losses/)\n",
    "\n",
    "See https://keras.io/models/model/ for how to compile, train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import ?\n",
    "\n",
    "# Set seed from random number generator, for better comparisons\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "def build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False):\n",
    "    \n",
    "    # Setup optimizer, depending on input parameter string\n",
    "    \n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model, using the input parameters of the build_DNN function\n",
    "    \n",
    "    # Add first layer, requires input shape\n",
    "    \n",
    "    # Add remaining layers, do not require input shape\n",
    "    for i in range(n_layers-1):\n",
    "        \n",
    "           \n",
    "    \n",
    "    \n",
    "    # Final layer\n",
    "    \n",
    "    \n",
    "    # Compile model\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a help function for plotting the training results\n",
    "\n",
    "# IMPORTANT NOTE\n",
    "# The history unfortunately behaves a bit randomly for every user\n",
    "# If the plots for accuracy and loss look mixed, change the order of\n",
    "# val_loss, val_acc, loss, acc\n",
    "# until the plots look as they \"should\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_results(history):\n",
    "    val_loss, val_acc, loss, acc = history.history.values()\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 11: Train the DNN\n",
    "\n",
    "Time to train the DNN, we start simple with 2 layers with 2 nodes each, learning rate 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "\n",
    "input_shape = ?\n",
    "\n",
    "# Build the model\n",
    "model1 = \n",
    "\n",
    "# Train the model, provide training data and validation data\n",
    "history1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12: More questions\n",
    "\n",
    "Question 9: What happens if you add several Dense layers without specifying the activation function?\n",
    "\n",
    "Question 10: How are the weights in each dense layer initialized as default? How are the bias weights initialized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13: Balancing the classes\n",
    "\n",
    "This dataset is rather unbalanced, we need to define class weights so that the training pays more attention to the class with fewer samples. We use a function in scikit learn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "\n",
    "\n",
    "# Print the class weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model2 = \n",
    "\n",
    "history2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 14: More questions\n",
    "\n",
    "Question 11: Why do we have to use a batch size? Why can't we simply use all data at once? This is more relevant for even larger datasets.\n",
    "\n",
    "Question 12: How busy is the GPU for a batch size of 100? How much GPU memory is used? Hint: run 'nvidia-smi' on the cloud computer a few times during training.\n",
    "\n",
    "Question 13: What is the processing time for one training epoch when the batch size is 100? What is the processing time for one epoch when the batch size is 1,000? What is the processing time for one epoch when the batch size is 10,000? Explain the results. \n",
    "\n",
    "Question 14: How many times are the weights in the DNN updated in each training epoch if the batch size is 100? How many times are the weights in the DNN updated in each training epoch if the batch size is 1,000? How many times are the weights in the DNN updated in each training epoch if the batch size is 10,000?  \n",
    "\n",
    "Question 15: What limits how large the batch size can be?\n",
    "\n",
    "Question 16: Generally speaking, how is the learning rate related to the batch size? If the batch size is decreased, how should the learning rate be changed?\n",
    "\n",
    "Lets use a batch size of 10,000 from now on, and a learning rate of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 15: Increasing the complexity\n",
    "\n",
    "Lets try some different configurations of number of layers and number of nodes per layer.\n",
    "\n",
    "Question 17: How many trainable parameters does the network with 4 dense layers with 50 nodes each have, compared to the initial network with 2 layers and 20 nodes per layer? Hint: use model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model3 = \n",
    "\n",
    "history3 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model4 = \n",
    "\n",
    "history4 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model5 = \n",
    "\n",
    "history5 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 16: Batch normalization\n",
    "\n",
    "Now add batch normalization after each dense layer. Remember to import BatchNormalization from keras.layers. \n",
    "\n",
    "See https://keras.io/layers/normalization/ for information about how to call the function.\n",
    "\n",
    "Question 18: Why is batch normalization important when training deep networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model6 = \n",
    "\n",
    "history6 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 17: Activation function\n",
    "\n",
    "Try changing the activation function in each layer from sigmoid to ReLU, write down the test accuracy.\n",
    "\n",
    "Note: the last layer should still have a sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, ReLU, no batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model7 = \n",
    "\n",
    "history7 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 18: Optimizer\n",
    "\n",
    "Try changing the optimizer from SGD to Adam (with learning rate 0.1 as before). Remember to import the Adam optimizer from keras.optimizers. \n",
    "\n",
    "https://keras.io/optimizers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, Adam optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model8 = \n",
    "\n",
    "history8 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 19: Dropout regularization\n",
    "\n",
    "Dropout is a type of regularization that can improve accuracy for validation and test data. \n",
    "\n",
    "Add a Dropout layer after each Dense layer (but not after the final dense layer), with a dropout probability of 50%. Remember to first import the Dropout layer from keras.layers\n",
    "\n",
    "See https://keras.io/layers/core/ for how the Dropout layer works.\n",
    "\n",
    "---\n",
    "\n",
    "Question 19: How does the validation accuracy change when adding dropout?\n",
    "\n",
    "Question 20: How does the test accuracy change when adding dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, dropout, SGD optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model9 = \n",
    "\n",
    "history9 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 20: Improving performance\n",
    "\n",
    "Spend some time (30 - 90 minutes) playing with the network architecture (number of layers, number of nodes per layer, activation function) and other hyper parameters (optimizer, learning rate, batch size, number of epochs, degree of regularization). For example, try a much deeper network. How much does the training time increase for a network with 10 layers?\n",
    "\n",
    "Question 21: How high classification accuracy can you achieve for the test data? What is your best configuration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find your best configuration for the DNN\n",
    "\n",
    "# Build and train DNN\n",
    "model10 = \n",
    "\n",
    "history10 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DNN on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 21: Dropout uncertainty\n",
    "\n",
    "Dropout can also be used during testing, to obtain an estimate of the model uncertainty. Since dropout will randomly remove connections, the network will produce different results every time the same (test) data is put into the network. This technique is called Monte Carlo dropout. For more information, see this paper http://proceedings.mlr.press/v48/gal16.pdf\n",
    "\n",
    "To achieve this, we need to redefine the Keras Dropout call by running the cell below, and use 'myDropout' in each call to Dropout, in the cell that defines the DNN.\n",
    "\n",
    "Run the same test data through the trained network 100 times, with dropout turned on. \n",
    "\n",
    "Question 22: What is the mean and the standard deviation of the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class myDropout(keras.layers.Dropout):\n",
    "    \"\"\"Applies Dropout to the input.\n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n",
    "           http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, training=True, noise_shape=None, seed=None, **kwargs):\n",
    "        super(myDropout, self).__init__(rate, noise_shape=None, seed=None,**kwargs)\n",
    "        self.training = training\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(inputs, self.rate, noise_shape,\n",
    "                                 seed=self.seed)\n",
    "            if not training: \n",
    "                return K.in_train_phase(dropped_inputs, inputs, training=self.training)\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your best config, custom dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your best training parameters\n",
    "\n",
    "\n",
    "# Build and train model\n",
    "model11 = \n",
    "\n",
    "history11 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell a few times to evalute the model on test data, \n",
    "# if you get slightly different test accuracy every time, Dropout during testing is working\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = \n",
    "                       \n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testing 100 times, and save the accuracies in an array\n",
    "\n",
    "    \n",
    "# Calculate and print mean and std of accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 22: Cross validation uncertainty\n",
    "\n",
    "Cross validation (CV) is often used to evaluate a model, by training and testing using different subsets of the data it is possible to get the uncertainty as the standard deviation over folds. We here use a help function from scikit-learn to setup the CV, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html . Use 10 folds with shuffling, random state 1234. \n",
    "\n",
    "Note: We here assume that you have found the best hyper parameters, so here the data are only split into training and testing, no validation.\n",
    "\n",
    "---\n",
    "\n",
    "Question 23: What is the mean and the standard deviation of the test accuracy?\n",
    "\n",
    "Question 24: What is the main advantage of dropout compared to CV for estimating test uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define 10-fold cross validation\n",
    "\n",
    "# Loop over cross validation folds\n",
    "    \n",
    "    # Calculate class weights for current split\n",
    "    \n",
    "    # Rebuild the DNN model, to not continue training on the previously trained model\n",
    "    \n",
    "    # Fit the model with training set and class weights for this fold\n",
    "    \n",
    "    # Evaluate the model using the test set for this fold\n",
    "    \n",
    "    # Save the test accuracy in an array\n",
    "\n",
    "# Calculate and print mean and std of accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 23: DNN regression\n",
    "\n",
    "A similar DNN can be used for regression, instead of classification.\n",
    "\n",
    "Question 25: How would you change the DNN in order to use it for regression instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "Send in this jupyter notebook, with answers to all questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
